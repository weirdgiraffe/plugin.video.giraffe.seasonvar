# coding: utf-8

#
# Copyright © 2017 weirdgiraffe <giraffe@cyberzoo.xyz>
#
# Distributed under terms of the MIT license.
#
import re
import base64

season_and_serial_re = re.compile(r'data-id-season="(\d+)"\s+data-id-serial="(\d+)"')

secure_and_time_re = re.compile(r"var\s+data4play\s*=\s*{\s*"
                                r"'secureMark'\s*:\s*'([a-f0-9]+)',\s*"
                                r"'time'\s*:\s*'([0-9]+)'\s*")

def main_page_items(main_page_html, datestr):
    '''Collect all dayblock items(i.e episode changes) for a given date
    and yields dict {'url': ..., 'name': ..., 'changes': ...} for each
    episode for that date.

    datestr should be in format 'dd.mm.yyyy'
    '''
    for (date, dayblock) in _main_page_dayblocks(main_page_html):
        if datestr == date:
            for item in _main_page_dayblock_items(dayblock):
                yield item


def search_items(search_response):
    '''yield dict {'name':..., 'url': ...} for items in search_response

    search_response is dict representation of search response
    '''
    try:
        r = re.compile(r'serial-\d+-[^.]+?\.html')
        suggestions = search_response['suggestions']
        data = search_response['data']
        for name, url in zip(suggestions['valu'], data):
            if url and r.match(url) is not None:
                yield {'name': name, 'url': '/' + url}
    except (KeyError, TypeError):
        return


def seasons(season_page_html):
    '''takes content of season page and yields
    all seasons for the same show.

    season_page_html should be utf-8 encoded html content
    '''
    r = re.compile(r'<h2>\s*<a\s+href="(/serial-\d+-[^.]+?\.html)"')
    for url in r.findall(season_page_html):
        yield url


def player_params(season_page_html):
    '''extract parameters for player.php to retrieve playlists
    if parameters not found return None
    '''
    sands = _season_and_serial(season_page_html)
    sandt = _secure_and_time(season_page_html)
    if sands and sandt:
        return {
                'id': sands[0],
                'serial': sands[1],
                'secure': sandt[0],
                'time': sandt[1],
                'type': 'html5',
        }


def playlists(player_response_html):
    '''takes response from player.php and yield dict {'tr':..., 'url':...}
    where 'tr' is a translation name and 'url' is a playlist url.
    If no translations found on page, then search for playlist urls only
    will be done. In this case translation names will be None.

    season_page_html should be utf-8 encoded html content
    '''
    r = re.compile(r'var pl = {\'0\': "(.+)"};')
    for url in r.findall(player_response_html):
        yield {'tr': None,
               'url': url.strip()}
    translations = _translate_list(player_response_html)
    if translations is not None:
        r = re.compile(r'<li data-click="translate[^>]*?>([^<]+)</li>[\s\n]*?'
                       '<script>pl\[\d+\] = "(.*?)";',
                       re.DOTALL)
        for name, url in r.findall(translations):
            yield {'tr': name.strip() if name != 'Стандартный' else None,
                   'url': url.strip()}


def episodes(playlist):
    '''yield dict {'name':..., 'url': ...} for items in playlist_dict

    playlist_html is response from requester.playlist()
    '''
    for entry in playlist:
        if 'playlist' in entry:
            for episode in entry['playlist']:
                yield {'url': _decode_episode_file_to_url(episode['file']),
                       'name': episode['title'].replace('<br>', ' ')}
        else:
            yield {'url': _decode_episode_file_to_url(entry['file']),
                   'name': entry['title'].replace('<br>', ' ')}


episode_link_preambula = '#2'
episode_link_trash = (r'\/\/b2xvbG8=', r'//b2xvbG8=')

def _decode_episode_file_to_url(enc_url):
    '''
    seasonvar.ru developers uses a trick to decode episode plain URL, i.e.
        http://data11-cdn.datalock.ru/fi2lm/0b621d3fbcbe74cfeb0fbe93d5dc1512/7f_Detstvo.Sheldona.KB.S6E02.a1.28.10.22.mp4
    is encoded as
        #2aHR0cDovL2RhdGExMS1jZG4uZGF0YWxvY2sucnUvZmkybG0vMGI2MjFkM2ZiY2JlNzRjZmViMGZiZTkzZDVkYzE1MTIvN2ZfRGV0c3R2by5TaGVsZG9uYS5LQi5TNkUwMi5hMS4yOC4xMC\/\/b2xvbG8=4yMi5tcDQ=
    Rules are simple:
    * remove preambule '#2'
    * remove trash '\/\/b2xvbG8='
    * decode from base64
    '''
    enc_url = enc_url.replace(episode_link_preambula, '', 1)
    for trash in episode_link_trash:
        enc_url = enc_url.replace(trash, '', 1)
    try:
        return base64.b64decode(enc_url).decode('utf-8')
    except:
        import pdb; pdb.set_trace()
        return ''

def _translate_list(season_page_html):
    r = re.compile(r'<ul class="pgs-trans"(.*?)</ul>', re.DOTALL)
    for b in r.findall(season_page_html):
        return b


def _season_and_serial(season_page_html):
    m = season_and_serial_re.search(season_page_html)
    return m and m.groups() or None


def _secure_and_time(season_page_html):
    m = secure_and_time_re.search(season_page_html)
    return m and m.groups() or None


def _main_page_dayblocks(full_page_html):
    '''Collect all dayblocks from full_page_html
    and yield (date, content) for every dayblock'''
    r = re.compile(
        r'<div class="news-head">\s*?(\d{2}\.\d{2}\.\d{4})(.*?)'
        r'(?=<div class="doptxt"|<div class="news")',
        re.DOTALL)
    for group in r.findall(full_page_html):
        d, c = group
        yield group


def _main_page_dayblock_items(dayblock_content):
    '''Collect all series items from dayblock_content
    and for every item yield dict with description'''
    r = re.compile(
        r'<a href="(\/serial-.+?\.html)"[^>]*?>'
        '.*?div.*?<div[^>]*?>(.+?)<\/div>'
        '(.*?)<span[^>]*?>(.+?)<\/span>.*?<\/a>',
        re.DOTALL)
    for (url, name, season, changes) in r.findall(dayblock_content):
        changes = season.strip() + ' ' + changes.strip()
        name = name.replace("<span>", "")
        name = name.replace("</span>", "")
        yield {'url': url,
               'name': name.strip(),
               'changes': changes.strip()}
